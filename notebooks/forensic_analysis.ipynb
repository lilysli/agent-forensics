{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fba4543",
   "metadata": {},
   "source": [
    "# Forensic analysis tool\n",
    "\n",
    "This tool helps compare the logging files from the benign and the triggered agent.\n",
    "It makes four analyses:\n",
    "- Steps analysis: Step duration, reasoning size, answer size, total steps\n",
    "- Tools analysis: Tool calls, command uses, sizes of files written (write size)\n",
    "- States: Total changes, total file changes, total folders changes\n",
    "- Errors: Total errors, repeated errors, repeated errors right after each other (max streak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3bcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b74047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared parsing and metrics helpers\n",
    "def parse_csv(path: str, required_cols: list) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    missing = set(required_cols) - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in {path}: {missing}\")\n",
    "    return df\n",
    "\n",
    "def safe_stat(series, func, default=None):\n",
    "    s = pd.to_numeric(series, errors='coerce').dropna()\n",
    "    return func(s) if len(s) > 0 else default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9d423",
   "metadata": {},
   "source": [
    "### Steps analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_COLS = [\"step_count\",\"step_start_ts\",\"step_end_ts\",\n",
    "             \"reasoning_content\",\"reasoning_size\",\"answer_content\",\"answer_size\"]\n",
    "\n",
    "def load_steps(path: str) -> pd.DataFrame:\n",
    "    df = parse_csv(path, STEP_COLS)\n",
    "    df[\"step_start_ts\"] = pd.to_datetime(df[\"step_start_ts\"], errors=\"coerce\")\n",
    "    df[\"step_end_ts\"] = pd.to_datetime(df[\"step_end_ts\"], errors=\"coerce\")\n",
    "    df[\"duration_sec\"] = (df[\"step_end_ts\"] - df[\"step_start_ts\"]).dt.total_seconds()\n",
    "    return df\n",
    "\n",
    "def steps_metrics(df: pd.DataFrame) -> dict:\n",
    "    dur = df[\"duration_sec\"].dropna()\n",
    "    m = {\n",
    "        \"total_steps\": len(df),\n",
    "        \"min_reasoning_size\": int(df[\"reasoning_size\"].min()),\n",
    "        \"avg_reasoning_size\": float(df[\"reasoning_size\"].mean()),\n",
    "        \"max_reasoning_size\": int(df[\"reasoning_size\"].max()),\n",
    "        \"min_answer_size\": int(df[\"answer_size\"].min()),\n",
    "        \"avg_answer_size\": float(df[\"answer_size\"].mean()),\n",
    "        \"max_answer_size\": int(df[\"answer_size\"].max()),\n",
    "    }\n",
    "    if len(dur) > 0:\n",
    "        m.update({\n",
    "            \"min_duration_sec\": float(dur.min()),\n",
    "            \"avg_duration_sec\": float(dur.mean()),\n",
    "            \"max_duration_sec\": float(dur.max()),\n",
    "            \"min_duration_step\": int(df.loc[dur.idxmin(), \"step_count\"]),\n",
    "            \"max_duration_step\": int(df.loc[dur.idxmax(), \"step_count\"]),\n",
    "        })\n",
    "    return m\n",
    "\n",
    "def steps_analysis(a: str, b: str):\n",
    "    dfs = [load_steps(a), load_steps(b)]\n",
    "    names = [os.path.basename(a), os.path.basename(b)]\n",
    "    metrics = [steps_metrics(df) for df in dfs]\n",
    "    summary = pd.DataFrame([{**m, \"file\": n} for m, n in zip(metrics, names)])\n",
    "    \n",
    "    display(Markdown(\"## Steps Analysis\"))\n",
    "    display(Markdown(\"### Per-file metrics\"))\n",
    "    display(summary)\n",
    "    return dict(zip([\"file_a\", \"file_b\"], metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76956dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps visualisation\n",
    "\n",
    "def _bar(ax, labels, va, vb, title, a_name, b_name, anns=None):\n",
    "    x = np.arange(len(labels))\n",
    "    w = 0.38\n",
    "    ax.bar(x - w/2, va, w, label=a_name)\n",
    "    ax.bar(x + w/2, vb, w, label=b_name)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, axis='y', alpha=0.25)\n",
    "    if anns:\n",
    "        for i, ann in enumerate(anns):\n",
    "            if ann:\n",
    "                ax.text(x[i], max(va[i], vb[i], 0), ann, ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "def steps_plots(a: str, b: str):\n",
    "    dfs = [load_steps(a), load_steps(b)]\n",
    "    names = [os.path.basename(a), os.path.basename(b)]\n",
    "    met = [steps_metrics(df) for df in dfs]\n",
    "    \n",
    "    dur_labels = [\"min\", \"avg\", \"max\"]\n",
    "    dur_a = [m.get(\"min_duration_sec\", 0) for m in met]\n",
    "    dur_b = [m.get(\"avg_duration_sec\", 0) for m in met]\n",
    "    dur_b = [m.get(\"max_duration_sec\", 0) for m in met]  # fix typo\n",
    "    dur_b = [m.get(\"max_duration_sec\", 0) for m in met]\n",
    "    dur_vals = [[m.get(k, 0) for k in [\"min_duration_sec\", \"avg_duration_sec\", \"max_duration_sec\"]] for m in met]\n",
    "    dur_a, dur_b = dur_vals[0], dur_vals[1]\n",
    "    anns = [\n",
    "        f\"step {met[0].get('min_duration_step')} vs {met[1].get('min_duration_step')}\" if all(m.get('min_duration_step') for m in met) else None,\n",
    "        None,\n",
    "        f\"step {met[0].get('max_duration_step')} vs {met[1].get('max_duration_step')}\" if all(m.get('max_duration_step') for m in met) else None,\n",
    "    ]\n",
    "\n",
    "    reason_vals = [[m[k] for k in [\"min_reasoning_size\",\"avg_reasoning_size\",\"max_reasoning_size\"]] for m in met]\n",
    "    answer_vals = [[m[k] for k in [\"min_answer_size\",\"avg_answer_size\",\"max_answer_size\"]] for m in met]\n",
    "    steps_vals = [[m[\"total_steps\"]] for m in met]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    _bar(axs[0,0], dur_labels, dur_a, dur_b, \"Step Duration (sec)\", *names, anns)\n",
    "    _bar(axs[0,1], dur_labels, *reason_vals, \"Reasoning Size (chars)\", *names)\n",
    "    _bar(axs[1,0], dur_labels, *answer_vals, \"Answer Size (chars)\", *names)\n",
    "    _bar(axs[1,1], [\"total\"], *steps_vals, \"Total Steps\", *names)\n",
    "    fig.suptitle(\"Steps Comparison\", fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf5602",
   "metadata": {},
   "source": [
    "### Tools analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd16fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools analysis\n",
    "\n",
    "TOOL_COLS = [\"ts\",\"tool_name\",\"command\",\"parameters_json\",\"affected_path\",\"size_bytes\"]\n",
    "\n",
    "def load_tools(path: str) -> pd.DataFrame:\n",
    "    return parse_csv(path, TOOL_COLS)\n",
    "\n",
    "def tools_metrics(df: pd.DataFrame) -> dict:\n",
    "    writes = df[df[\"command\"] == \"write\"][\"size_bytes\"]\n",
    "    write_sizes = pd.to_numeric(writes, errors=\"coerce\").dropna()\n",
    "    return {\n",
    "        \"total_calls\": len(df),\n",
    "        \"unique_tools\": df[\"tool_name\"].nunique(),\n",
    "        \"unique_commands\": df[\"command\"].nunique(),\n",
    "        \"tool_counts\": df[\"tool_name\"].value_counts().sort_index(),\n",
    "        \"command_counts\": df[\"command\"].value_counts().sort_index(),\n",
    "        \"write_metrics\": {\n",
    "            \"count\": len(write_sizes),\n",
    "            \"min\": int(write_sizes.min()) if len(write_sizes) > 0 else None,\n",
    "            \"avg\": float(write_sizes.mean()) if len(write_sizes) > 0 else None,\n",
    "            \"max\": int(write_sizes.max()) if len(write_sizes) > 0 else None,\n",
    "        }\n",
    "    }\n",
    "\n",
    "def tools_analysis(a: str, b: str):\n",
    "    dfs = [load_tools(a), load_tools(b)]\n",
    "    names = [os.path.basename(a), os.path.basename(b)]\n",
    "    met = [tools_metrics(df) for df in dfs]\n",
    "    \n",
    "    summary = pd.DataFrame([{\n",
    "        \"file\": n,\n",
    "        \"total_calls\": m[\"total_calls\"],\n",
    "        \"unique_tools\": m[\"unique_tools\"],\n",
    "        \"unique_commands\": m[\"unique_commands\"],\n",
    "    } for m, n in zip(met, names)])\n",
    "    \n",
    "    tool_comp = pd.DataFrame({names[0]: met[0][\"tool_counts\"], names[1]: met[1][\"tool_counts\"]}).fillna(0).astype(int)\n",
    "    cmd_comp = pd.DataFrame({names[0]: met[0][\"command_counts\"], names[1]: met[1][\"command_counts\"]}).fillna(0).astype(int)\n",
    "    write_sum = pd.DataFrame([{\"file\": n, **m[\"write_metrics\"]} for m, n in zip(met, names)])\n",
    "\n",
    "    display(Markdown(\"## Tools Analysis\"))\n",
    "    display(Markdown(\"### Summary\"))\n",
    "    display(summary)\n",
    "    display(Markdown(\"### Tool Calls\"))\n",
    "    display(tool_comp)\n",
    "    display(Markdown(\"### Commands\"))\n",
    "    display(cmd_comp)\n",
    "    display(Markdown(\"### Write Size (bytes)\"))\n",
    "    display(write_sum)\n",
    "    \n",
    "    return {\"file_a\": met[0], \"file_b\": met[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbe122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools visualisation\n",
    "\n",
    "def align_series(s1, s2, top_n=10):\n",
    "    df = pd.DataFrame({\n",
    "        \"A\": s1,\n",
    "        \"B\": s2,\n",
    "    }).fillna(0)\n",
    "    with pd.option_context('mode.use_inf_as_na', True):\n",
    "        df = df.fillna(0).astype(int)\n",
    "\n",
    "    df[\"total\"] = df[\"A\"] + df[\"B\"]\n",
    "    df = df.sort_values(\"total\", ascending=False).drop(columns=[\"total\"]) \n",
    "    if top_n is not None:\n",
    "        df = df.head(top_n)\n",
    "\n",
    "    if len(df) == 0 or df.shape[1] < 2:\n",
    "        return [], [], []\n",
    "\n",
    "    labels = df.index.tolist()\n",
    "    vals_a = df[\"A\"].tolist()\n",
    "    vals_b = df[\"B\"].tolist()\n",
    "    return labels, vals_a, vals_b\n",
    "    \n",
    "\n",
    "def tools_plots(a: str, b: str, top_n=10):\n",
    "    dfs = [load_tools(a), load_tools(b)]\n",
    "    names = [os.path.basename(a), os.path.basename(b)]\n",
    "    met = [tools_metrics(df) for df in dfs]\n",
    "    \n",
    "    # Top tools\n",
    "    t_labels, t_a, t_b = align_series(met[0][\"tool_counts\"], met[1][\"tool_counts\"], top_n)\n",
    "    # Top commands\n",
    "    c_labels, c_a, c_b = align_series(met[0][\"command_counts\"], met[1][\"command_counts\"], top_n)\n",
    "    # Write size\n",
    "    w_labels = [\"min\", \"avg\", \"max\"]\n",
    "    w_a = [m for m in met[0][\"write_metrics\"].values()][1:]  \n",
    "    w_b = [m for m in met[1][\"write_metrics\"].values()][1:]\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 9))\n",
    "    _bar(axs[0], t_labels, t_a, t_b, \"Tool Calls\", *names)\n",
    "    _bar(axs[1], c_labels, c_a, c_b, \"Command Uses\", *names)\n",
    "    _bar(axs[2], w_labels, w_a, w_b, \"Write Size (bytes)\", *names)\n",
    "    fig.suptitle(\"Tools Comparison\", fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80358dd",
   "metadata": {},
   "source": [
    "### States analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_COLS = [\"ts\",\"state\",\"count_folders\",\"count_files\"]\n",
    "\n",
    "def load_states(path: str) -> pd.DataFrame:\n",
    "    df = parse_csv(path, STATE_COLS)\n",
    "    for c in [\"count_folders\", \"count_files\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "def states_metrics(df: pd.DataFrame) -> dict:\n",
    "    d_files = df[\"count_files\"].diff().fillna(0).astype(int)\n",
    "    d_dirs = df[\"count_folders\"].diff().fillna(0).astype(int)\n",
    "    return {\n",
    "        \"total_changes\": len(df),\n",
    "        \"total_files_changes\": int(d_files.sum()),\n",
    "        \"min_files_changes_per_step\": int(d_files.min()),\n",
    "        \"max_files_changes_per_step\": int(d_files.max()),\n",
    "        \"total_folders_changes\": int(d_dirs.sum()),\n",
    "        \"min_folders_changes_per_step\": int(d_dirs.min()),\n",
    "        \"max_folders_changes_per_step\": int(d_dirs.max()),\n",
    "        \"state_counts\": df[\"state\"].value_counts().sort_index(),\n",
    "    }\n",
    "\n",
    "def states_analysis(a: str, b: str):\n",
    "    dfs = [load_states(a), load_states(b)]\n",
    "    names = [os.path.basename(a), os.path.basename(b)]\n",
    "    met = [states_metrics(df) for df in dfs]\n",
    "    \n",
    "    summary = pd.DataFrame([{**{k: v for k, v in m.items() if k != \"state_counts\"}, \"file\": n}\n",
    "                            for m, n in zip(met, names)])\n",
    "    states_comp = pd.DataFrame({names[0]: met[0][\"state_counts\"], names[1]: met[1][\"state_counts\"]}).fillna(0).astype(int)\n",
    "\n",
    "    display(Markdown(\"## States Analysis\"))\n",
    "    display(Markdown(\"### Summary\"))\n",
    "    display(summary)\n",
    "    display(Markdown(\"### State Label Counts\"))\n",
    "    display(states_comp)\n",
    "\n",
    "    return {\"summary\": summary, \"states_compare\": states_comp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# States visualisation\n",
    "\n",
    "def states_plots(a: str, b: str):\n",
    "    dfs = [load_states(a), load_states(b)]\n",
    "    names = [os.path.basename(a), os.path.basename(b)]\n",
    "    met = [states_metrics(df) for df in dfs]\n",
    "    \n",
    "    labels = [\"total_changes\", \"total_files_changes\", \"total_folders_changes\"]\n",
    "    vals_a = [met[0][\"total_changes\"], met[0][\"total_files_changes\"], met[0][\"total_folders_changes\"]]\n",
    "    vals_b = [met[1][\"total_changes\"], met[1][\"total_files_changes\"], met[1][\"total_folders_changes\"]]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    _bar(ax, labels, vals_a, vals_b, \"States Comparison\", *names)\n",
    "    fig.suptitle(\"States Comparison\", fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df68a457",
   "metadata": {},
   "source": [
    "### Errors analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_COLS = [\"ts\",\"tool_name\",\"command\",\"parameters_json\",\"description\"]\n",
    "\n",
    "def load_errors(path: str) -> pd.DataFrame:\n",
    "    df = parse_csv(path, ERROR_COLS)\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\")\n",
    "    return df.sort_values(\"ts\").reset_index(drop=True)\n",
    "\n",
    "def errors_metrics(df: pd.DataFrame) -> dict:\n",
    "    # Streak detection: consecutive identical (tool, command)\n",
    "    same = (df[\"tool_name\"] == df[\"tool_name\"].shift()) & (df[\"command\"] == df[\"command\"].shift())\n",
    "    streak_id = (~same).cumsum()\n",
    "    streaks = df.groupby(streak_id).size()\n",
    "    repeated = streaks[streaks >= 2]\n",
    "    \n",
    "    return {\n",
    "        \"total_errors\": len(df),\n",
    "        \"description_counts\": df[\"description\"].value_counts().sort_index(),\n",
    "        \"repeats_count\": int((streaks - 1).clip(0).sum()),\n",
    "        \"sequences_count\": int(len(repeated)),\n",
    "        \"max_streak\": int(streaks.max()) if len(streaks) else 0,\n",
    "    }\n",
    "\n",
    "def errors_analysis(a: str, b: str):\n",
    "    dfs = [load_errors(a), load_errors(b)]\n",
    "    names = [os.path.basename(a), os.path.basename(b)]\n",
    "    met = [errors_metrics(df) for df in dfs]\n",
    "    \n",
    "    summary = pd.DataFrame([{\n",
    "        \"file\": n,\n",
    "        \"total_errors\": m[\"total_errors\"],\n",
    "        \"repeats_count\": m[\"repeats_count\"],\n",
    "        \"sequences_count\": m[\"sequences_count\"],\n",
    "        \"max_streak\": m[\"max_streak\"],\n",
    "    } for m, n in zip(met, names)])\n",
    "    \n",
    "    desc_comp = pd.DataFrame({names[0]: met[0][\"description_counts\"], names[1]: met[1][\"description_counts\"]}).fillna(0).astype(int)\n",
    "\n",
    "    display(Markdown(\"## Errors Analysis\"))\n",
    "    display(Markdown(\"### Summary\"))\n",
    "    display(summary)\n",
    "    display(Markdown(\"### Error Counts by Description\"))\n",
    "    display(desc_comp)\n",
    "\n",
    "    return {\"summary\": summary, \"desc_compare\": desc_comp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94abf864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors plots\n",
    "\n",
    "def errors_plots(a: str, b: str, top_n=10):\n",
    "    dfs = [load_errors(a), load_errors(b)]\n",
    "    names = [os.path.basename(a), os.path.basename(b)]\n",
    "    met = [errors_metrics(df) for df in dfs]\n",
    "    \n",
    "    # Totals\n",
    "    labels = [\"total\", \"repeats\", \"max_streak\"]\n",
    "    vals_a = [met[0][\"total_errors\"], met[0][\"repeats_count\"], met[0][\"max_streak\"]]\n",
    "    vals_b = [met[1][\"total_errors\"], met[1][\"repeats_count\"], met[1][\"max_streak\"]]\n",
    "    \n",
    "    # Top error descriptions\n",
    "    desc_df = pd.DataFrame({names[0]: met[0][\"description_counts\"], names[1]: met[1][\"description_counts\"]}).fillna(0)\n",
    "    desc_df = desc_df.sort_values(desc_df.columns.tolist(), ascending=False).head(top_n)\n",
    "    d_labels, d_a, d_b = desc_df.index.tolist(), desc_df.iloc[:,0].tolist(), desc_df.iloc[:,1].tolist()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    _bar(axs[0], labels, vals_a, vals_b, \"Errors Summary\", *names)\n",
    "    _bar(axs[1], d_labels, d_a, d_b, \"Top Error Descriptions\", *names)\n",
    "    fig.suptitle(\"Errors Comparison\", fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2334d5",
   "metadata": {},
   "source": [
    "### Auto-run analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df897171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_logs_dir(hint=\"results/logs\") -> str | None:\n",
    "    if os.path.isabs(hint) and os.path.isdir(hint):\n",
    "        return hint\n",
    "    for p in [\".\", \"..\", \"../..\", \"../../..\", \"../../../..\"]:\n",
    "        cand = os.path.abspath(os.path.join(p, hint))\n",
    "        if os.path.isdir(cand):\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "def latest(pattern: str) -> str | None:\n",
    "    matches = glob.glob(pattern)\n",
    "    return max(matches, key=os.path.getmtime) if matches else None\n",
    "\n",
    "def find_pairs(exp: str, logs_dir=\"results/logs\"):\n",
    "    base = find_logs_dir(logs_dir)\n",
    "    if not base:\n",
    "        raise FileNotFoundError(f\"Logs dir not found (hint: {logs_dir})\")\n",
    "    types = [\"steps\", \"tools\", \"states\", \"errors\"]\n",
    "    return {t: (\n",
    "        latest(os.path.join(base, f\"{exp}-{t}-*.csv\")),\n",
    "        latest(os.path.join(base, f\"{exp}-triggered-{t}-*.csv\"))\n",
    "    ) for t in types}\n",
    "\n",
    "def run_all(exp: str, logs_dir=\"results/logs\"):\n",
    "    pairs = find_pairs(exp, logs_dir)\n",
    "    display(Markdown(f\"## Experiment: `{exp}`\"))\n",
    "    \n",
    "    def try_run(name, analyze, plot, pair):\n",
    "        a, b = pair\n",
    "        if not (a and b):\n",
    "            print(f\"Skipping {name}: missing files\")\n",
    "            return None\n",
    "        res = analyze(a, b)\n",
    "        fig = plot(a, b)\n",
    "        plt.show()\n",
    "        return res\n",
    "\n",
    "    return {\n",
    "        \"steps\":   try_run(\"Steps\",   steps_analysis,   steps_plots,   pairs[\"steps\"]),\n",
    "        \"tools\":   try_run(\"Tools\",   tools_analysis,   tools_plots,   pairs[\"tools\"]),\n",
    "        \"states\":  try_run(\"States\",  states_analysis,  states_plots,  pairs[\"states\"]),\n",
    "        \"errors\":  try_run(\"Errors\",  errors_analysis,  errors_plots,  pairs[\"errors\"]),\n",
    "        \"pairs\": pairs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all(\"go-game\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-forensics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
